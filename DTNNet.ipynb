{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DTNNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jHBo2RS8JZ4q0rmkRh2z_LvRDzEoiFO3",
      "authorship_tag": "ABX9TyOIY0/pXmLpWMniFsNRjnRs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84ef659c1a77492cb4651d1b9ae7c0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee946322de484005ba502882a30cbcfe",
              "IPY_MODEL_fade7e36426a456380494c27e8a715cf",
              "IPY_MODEL_45ffb9ac9633487f890482b5c21455e6"
            ],
            "layout": "IPY_MODEL_026200c3d5ae420b97c671fc6d38a7c0"
          }
        },
        "ee946322de484005ba502882a30cbcfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_914b6c9de95b47f7bde0af612f738311",
            "placeholder": "​",
            "style": "IPY_MODEL_344d3ee23fd649c0a6b9e886294a9e83",
            "value": "100%"
          }
        },
        "fade7e36426a456380494c27e8a715cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28fc4701dc61402bb47af877f2848d38",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70cc1110af3a42d7a0e3b8138572327e",
            "value": 553433881
          }
        },
        "45ffb9ac9633487f890482b5c21455e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a31e0ab35d5544e8b87d7c03f3705f37",
            "placeholder": "​",
            "style": "IPY_MODEL_e262c69328f04661a1ec34f8bc34cc48",
            "value": " 528M/528M [00:04&lt;00:00, 145MB/s]"
          }
        },
        "026200c3d5ae420b97c671fc6d38a7c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914b6c9de95b47f7bde0af612f738311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "344d3ee23fd649c0a6b9e886294a9e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28fc4701dc61402bb47af877f2848d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70cc1110af3a42d7a0e3b8138572327e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a31e0ab35d5544e8b87d7c03f3705f37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e262c69328f04661a1ec34f8bc34cc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Unrickyelenting/Crowd-counting/blob/main/DTNNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JitQQjBFD_jR"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import scipy.io as io\n",
        "import PIL.Image as Image\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter \n",
        "import scipy.spatial\n",
        "import json\n",
        "from matplotlib import cm as CM\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import random\n",
        "from PIL import Image,ImageFilter,ImageDraw\n",
        "from PIL import ImageStat\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DTNet(nn.Module):\n",
        "    def __init__(self,load_weights=False):\n",
        "        super(DTNet, self).__init__()\n",
        "\n",
        "        # image resolution. In order to get the embedding dimension\n",
        "\n",
        "\n",
        "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
        "        self.backend_feat  = [512, 512, 512,256,128,64,'T']\n",
        "        self.frontend = make_layers(self.frontend_feat,in_channels=3,dilation=False)\n",
        "\n",
        "        #calculate the new resolutiona after the maxpooling layer, because there are\n",
        "        # three max-pooling layer in the front-end it will become 1/8\n",
        "\n",
        "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "        if not load_weights:\n",
        "            mod = models.vgg16(pretrained = True)\n",
        "            self._initialize_weights()\n",
        "            value_list=[]\n",
        "            for k,v in mod.state_dict().items():\n",
        "                value_list.append(v.data[:])\n",
        "            index=0\n",
        "            for k,v in self.frontend.state_dict().items():\n",
        "                v.data[:] =value_list[index]\n",
        "                index=index+1\n",
        "    def forward(self,x):\n",
        "        x = self.frontend(x)\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "                \n",
        "def make_layers(cfg, in_channels = 3,dilation = False):\n",
        "    if dilation:\n",
        "        d_rate = 2\n",
        "    else:\n",
        "        d_rate = 1\n",
        "    layers = []\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            # when it is a maxpooling layer\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif v == 'T':\n",
        "            # # when it is a multihead self-attention layer\n",
        "            # multi_attention=MultiHeadAttention(n_head=1, d_model=in_channels, d_k=in_channels, d_v=in_channels)\n",
        "            layers += [MHSA(in_channels,heads=1)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class MHSA(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, n_dims, heads=1):\n",
        "        super(MHSA, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.dim=n_dims\n",
        "\n",
        "        self.multihead_attn=nn.MultiheadAttention(embed_dim=n_dims , num_heads=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Input x is a (n_batch, in_channels, width, height)\n",
        "        n_batch, C, width, height = x.size()\n",
        "        sequence_to_image=x\n",
        "        sequence_to_image=sequence_to_image.view(n_batch, C, -1).permute(0,2,1)\n",
        "    \n",
        "\n",
        "        # multihead_attn = nn.MultiheadAttention(n_head=self.heads,d_model=C,d_k=width*height,d_v=width*height)\n",
        "        # .to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        mymultihead_attn=self.multihead_attn\n",
        "        \n",
        "        attn_output, attn_output_weights = mymultihead_attn(sequence_to_image, sequence_to_image, sequence_to_image)\n",
        "\n",
        "        # skip connection\n",
        "        out=x+attn_output.view(n_batch, C, width, height)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "4oWOac14ENgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_4n88an9yH5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "# import h5py\n",
        "# import torch\n",
        "import shutil\n",
        "\n",
        "def save_net(fname, net):\n",
        "    with h5py.File(fname, 'w') as h5f:\n",
        "        for k, v in net.state_dict().items():\n",
        "            h5f.create_dataset(k, data=v.cpu().numpy())\n",
        "def load_net(fname, net):\n",
        "    with h5py.File(fname, 'r') as h5f:\n",
        "        for k, v in net.state_dict().items():        \n",
        "            param = torch.from_numpy(np.asarray(h5f[k]))         \n",
        "            v.copy_(param)\n",
        "            \n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')   \n",
        "\n",
        "# The input datset\n",
        "class listDataset(Dataset):\n",
        "    def __init__(self, root, shape=None, shuffle=True, transform=None,  train=False, seen=0, batch_size=1, num_workers=1):\n",
        "        #split the image into 4*4\n",
        "        if train:\n",
        "            root = root *4\n",
        "        random.shuffle(root)\n",
        "        \n",
        "        self.nSamples = len(root)\n",
        "        self.lines = root\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        self.shape = shape\n",
        "        self.seen = seen\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "    def __getitem__(self, index):\n",
        "        assert index <= len(self), 'index range error' \n",
        "        \n",
        "        img_path = self.lines[index]\n",
        "        \n",
        "        img,target = load_data(img_path,self.train)\n",
        "        \n",
        "        #img = 255.0 * F.to_tensor(img)\n",
        "        \n",
        "        #img[0,:,:]=img[0,:,:]-92.8207477031\n",
        "        #img[1,:,:]=img[1,:,:]-95.2757037428\n",
        "        #img[2,:,:]=img[2,:,:]-104.877445883\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img,target\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# data input\n",
        "def load_data(img_path,train = True):\n",
        "    gt_path = img_path.replace('.jpg','.csv').replace('img','den')\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    target = genfromtxt(img_path, delimiter=',')\n",
        "\n",
        "    #crop the image into 4*4 and training it\n",
        "    crop_size = (int(img.size[0]/2),int(img.size[1]/2))\n",
        "    if train:\n",
        "      if random.randint(0,9) < 5:\n",
        "        dx = int(random.randint(0,1)*img.size[0]*1./2)\n",
        "        dy = int(random.randint(0,1)*img.size[1]*1./2)\n",
        "      else:\n",
        "        dx = int(random.random()*img.size[0]*1./2)\n",
        "        dy = int(random.random()*img.size[1]*1./2)\n",
        "\n",
        "      img = img.crop((dx,dy,crop_size[0]+dx,crop_size[1]+dy))\n",
        "      target = target[dy:crop_size[1]+dy,dx:crop_size[0]+dx]\n",
        "        \n",
        "      # if random.random()>0.8:\n",
        "      #   target = np.fliplr(target)\n",
        "      #   img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    \n",
        "      \n",
        "    target = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
        "    \n",
        "    \n",
        "    return img,target\n",
        "\n",
        "\n",
        "pre=None    \n",
        "best_prec1 = 1e6\n",
        "\n",
        "original_lr = 1e-7\n",
        "lr = 1e-7\n",
        "batch_size = 1\n",
        "momentum = 0.95\n",
        "decay = 5*1e-4\n",
        "start_epoch = 0\n",
        "epochs = 2000\n",
        "steps = [-1,1,100,150]\n",
        "scales= [1,1,1,1]\n",
        "workers = 10\n",
        "seed = time.time()\n",
        "print_freq = 30\n",
        "\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "# Please change the root and of part_B_final or part_A_final\n",
        "########################################################################################\n",
        "# set the root\n",
        "root = '/content/drive/MyDrive/part_A_final/'\n",
        "\n",
        "train_root=root+'train/img/'\n",
        "train_list=glob.glob(pathname=train_root+'*.jpg') #get the training path list\n",
        "\n",
        "val_root=root+'test/img/'\n",
        "val_list=glob.glob(pathname=val_root+'*.jpg')\n",
        "\n",
        "# Using cuda\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "torch.cuda.manual_seed(seed)\n",
        "    \n",
        "model = DTNet()\n",
        "# pre_train='/content/drive/MyDrive/ Model_best_last_layer/model_best_9.17.tar'\n",
        "# print(\"=> loading checkpoint '{}'\".format(pre_train))\n",
        "# checkpoint = torch.load(pre_train)\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# best_prec1 = checkpoint['best_prec1']\n",
        "# my_models = model.state_dict()\n",
        "# pre_models = list(checkpoint['state_dict'].items())\n",
        "# preweight_dict = {}\n",
        "\n",
        "# for i in range(len(pre_models)):\n",
        "#   prelayer_name, pre_weights = pre_models[i]\n",
        "#   preweight_dict[prelayer_name] = pre_weights\n",
        "\n",
        "# for layer_name, value in my_models.items():\n",
        "#   if 'module.'+layer_name in preweight_dict.keys() or layer_name in preweight_dict.keys():\n",
        "#     try:\n",
        "#       my_models[layer_name] = preweight_dict['module.'+layer_name]\n",
        "#     except:\n",
        "#       my_models[layer_name] = preweight_dict[layer_name]\n",
        "# model.load_state_dict(my_models)\n",
        "# print(\"=> loaded checkpoint '{}' (epoch {})\".format(pre_train, checkpoint['epoch']))   \n",
        "model = model.cuda()\n",
        "\n",
        "    \n",
        "criterion = nn.MSELoss(size_average=False).cuda()\n",
        "    \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
        "              momentum=momentum,\n",
        "              weight_decay=decay)\n",
        "\n",
        "# if pre:\n",
        "#     if os.path.isfile(pre):\n",
        "#         print(\"=> loading checkpoint '{}'\".format(args.pre))\n",
        "#         checkpoint = torch.load(pre)\n",
        "#         start_epoch = checkpoint['epoch']\n",
        "#         best_prec1 = checkpoint['best_prec1']\n",
        "#         model.load_state_dict(checkpoint['state_dict'])\n",
        "#         optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "#         print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "#               .format(pre, checkpoint['epoch']))\n",
        "#     else:\n",
        "#         print(\"=> no checkpoint found at '{}'\".format(pre))\n",
        "            \n",
        "def train(train_list, model, criterion, optimizer, epoch):\n",
        "    \n",
        "    losses = AverageMeter()\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    \n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "          listDataset(train_list,\n",
        "                       shuffle=True,\n",
        "                       transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "                   ]), \n",
        "                       train=True, \n",
        "                       batch_size=batch_size,\n",
        "                       num_workers=workers),\n",
        "        batch_size=batch_size)\n",
        "    print('epoch %d, processed %d samples, lr %.10f' % (epoch, epoch * len(train_loader.dataset), lr))\n",
        "    \n",
        "    model.train()\n",
        "    end = time.time()\n",
        "    \n",
        "    for i,(img, target)in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "        \n",
        "        img = img.cuda()\n",
        "        img = Variable(img)\n",
        "        output = model(img)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        target = target.type(torch.FloatTensor).unsqueeze(0).cuda()\n",
        "        target = Variable(target)\n",
        "        \n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        losses.update(loss.item(), img.size(0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()    \n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        \n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  .format(\n",
        "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses))\n",
        "    \n",
        "def validate(val_list, model, criterion):\n",
        "    print ('begin test')\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    listDataset(val_list,\n",
        "                shuffle=False,\n",
        "                transform=transforms.Compose([\n",
        "                transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225]),\n",
        "                ]),  train=False),\n",
        "    batch_size=batch_size)    \n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    mae = 0\n",
        "    \n",
        "    for i,(img, target) in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "            img = img.cuda()\n",
        "            img = Variable(img)\n",
        "            output = model(img)\n",
        "        \n",
        "            mae += abs(output.data.sum()-target.sum().type(torch.FloatTensor).cuda())\n",
        "        \n",
        "    mae = mae/len(test_loader)    \n",
        "    print(' * MAE {mae:.3f} '\n",
        "              .format(mae=mae))\n",
        "\n",
        "    return mae    \n",
        "        \n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    \n",
        "    \n",
        "    lr = original_lr\n",
        "    \n",
        "    for i in range(len(steps)):\n",
        "        \n",
        "        scale = scales[i] if i < len(scales) else 1\n",
        "        \n",
        "        \n",
        "        if epoch >= steps[i]:\n",
        "            lr = lr * scale\n",
        "            if epoch == steps[i]:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "        \n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "84ef659c1a77492cb4651d1b9ae7c0a9",
            "ee946322de484005ba502882a30cbcfe",
            "fade7e36426a456380494c27e8a715cf",
            "45ffb9ac9633487f890482b5c21455e6",
            "026200c3d5ae420b97c671fc6d38a7c0",
            "914b6c9de95b47f7bde0af612f738311",
            "344d3ee23fd649c0a6b9e886294a9e83",
            "28fc4701dc61402bb47af877f2848d38",
            "70cc1110af3a42d7a0e3b8138572327e",
            "a31e0ab35d5544e8b87d7c03f3705f37",
            "e262c69328f04661a1ec34f8bc34cc48"
          ]
        },
        "id": "RthvCk8EEcr2",
        "outputId": "8fee3b9a-34b9-48b6-8f8f-ab989f428391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84ef659c1a77492cb4651d1b9ae7c0a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training process"
      ],
      "metadata": {
        "id": "_qBlZqujErSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(start_epoch, epochs):\n",
        "        \n",
        "    adjust_learning_rate(optimizer, epoch) \n",
        "    train(train_list, model, criterion, optimizer, epoch)\n",
        "    prec1 = validate(val_list, model, criterion)\n",
        "        \n",
        "    is_best = prec1 < best_prec1\n",
        "    best_prec1 = min(prec1, best_prec1)\n",
        "    print(' * best MAE {mae:.3f} '\n",
        "          .format(mae=best_prec1))\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': pre,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_prec1': best_prec1,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }, is_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "c4eCCnM1EfS1",
        "outputId": "e29856f4-22ee-4aa2-edd6-41cfb17505ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, processed 0 samples, lr 0.0000001000\n",
            "Epoch: [0][0/1196]\tTime 1.307 (1.307)\tData 0.705 (0.705)\tLoss 0.6811 (0.6811)\t\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fc683afb3182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9a335bb548e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_list, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9a335bb548e5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#img = 255.0 * F.to_tensor(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9a335bb548e5>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(img_path, train)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mgt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ground_truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mgt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'density'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}